{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('dsu-pytorch-onnx': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8687cf727fdb2cf639c7804b24accbc0bb7cbce87cf92e7a3fd2b399ab2527a1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Example 2: PyTorch\n",
    "\n",
    "#### Note: I recommend using Google Colab with GPU runtime if you plan to train your own models. Model training on CPU takes some time. \n",
    "\n",
    "### Install dependencies\n",
    "\n",
    "Install dependencies - use pip install -r requirements.txt.\n",
    "\n",
    "Please check the docs here: https://docs.fast.ai/tutorial.vision.html\n",
    "\n",
    "There's a full explanation to all steps to train DL model using FastAI. I'll try to keep it high level."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Training a DL model (Optional)\n",
    "\n",
    "### Dataset\n",
    "\n",
    "\n",
    "\n",
    "1. Dataset used to train the model https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog/data. For context https://www.youtube.com/watch?v=6ViobQys1iQ\n",
    "\n",
    "### Models\n",
    "\n",
    "1. Trained Resnet50 models here (if you do not wish to train your own model): \n",
    "        \n",
    "        https://storage.googleapis.com/dsu-models-20020301/example-2-pytorch/hot_dog_resnet50_256_256.onnx\n",
    "        https://storage.googleapis.com/dsu-models-20020301/example-2-pytorch/hot_dog_resnet50_256_256.pkl\n",
    "\n",
    "2. Training your own models - If you'd like to train a model on your own dataset please keep the folder setup - each folder and images in the folder should be named the indended 'label'. This is due FastAI DataBlock implementation. Check https://docs.fast.ai/ for more advanced data loading examples. See the folder structure below:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dataset\n",
    "├───hot_dog\n",
    "│   ├───hot_dot_1.png\n",
    "│   ├───hot_dot_2.png\n",
    "│   └───hot_dot_xyz.png\n",
    "└───not_hot_dog\n",
    "    ├───not_hot_dot_1.png\n",
    "    ├───not_hot_dot_2.png\n",
    "    └───not_hot_dot_xyz.png"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = DataBlock(blocks=(ImageBlock, CategoryBlock), \n",
    "                 get_items=get_image_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "                 item_tfms=Resize(460),\n",
    "                 batch_tfms=aug_transforms(size=256)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dogs.dataloaders(PATH) # GPU implementation\n",
    "dls = dogs.dataloaders(PATH, num_workers=0) # CPU implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show random images from the batch\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a convolutional learner\n",
    "# you can find other supported models here:\n",
    "# please stick to resnet models - resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "# in general more layers mean longer training and better performance\n",
    "\n",
    "learn = cnn_learner(dls, resnet50, metrics=[error_rate, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the best learning rate\n",
    "# Check this article for the maths behind this: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to change the number of training epochs - 1. argument\n",
    "# you should adjust the learning rate based on the previous step - 2. argument\n",
    "\n",
    "learn.fine_tune(5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the FastAI model\n",
    "\n",
    "learn.export('models/my_own_hot_dog_resnet50_256_256.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the FastAI model\n",
    "learn = load_learner('models/hot_dog_resnet50_256_256.pkl') # loading trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['hot_dog', 'not_hot_dog']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# exporting labels from dataloaders\n",
    "# FastAI dataloader gives quick access to labes\n",
    "\n",
    "labels = learn.dls.vocab\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%time\n",
    "# predicting with FastAI\n",
    "# FastAIs models already contains a wraper including the softmax layers\n",
    "learn.predict('dataset/hot_dog/hot_dog_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # getting the PyTorch model\n",
    " # by using .model attribute on the FastAI learned we can get the 'pure' PyTorch model\n",
    " # by using eval() we are setting the model to 'prediction' mode - no backward propagation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_model = learn.model.eval()\n",
    "softmax_layer = torch.nn.Softmax(dim=1) # Resnet models from PyTorch hub are without the last SoftMax layer (FastAI models already include this, also the image transformation - image -> tensor, resize to 256)\n",
    "\n",
    "final_model = nn.Sequential(fastai_model, softmax_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# loading an image and converting to tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open('dataset/hot_dog/hot_dog_1.jpg')\n",
    "\n",
    "# creating a transformation pipeline\n",
    "transformation = transforms.Compose([\n",
    "            transforms.Resize([256,256]),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "image_tensor = transformation(image).unsqueeze(0)\n",
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 178 ms, sys: 19.1 ms, total: 197 ms\nWall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    results = final_model(image_tensor)"
   ]
  },
  {
   "source": [
    "## Get label and probabilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('hot_dog', array([[0.9486536 , 0.05134634]], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "labels[np.argmax(results.detach().numpy())], results.detach().numpy()\n"
   ]
  },
  {
   "source": [
    "# Converting to ONNX\n",
    "\n",
    "We are converting the final model that already includes the softmax layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    final_model,\n",
    "    torch.randn(1, 3, 224, 224),\n",
    "    \"models/my_own_hot_dog_resnet50_256_256.onnx\",\n",
    "    export_params=True,\n",
    "    input_names=[\"image_256_256\"],\n",
    "    output_names=[\"hot_dog\"],\n",
    "    opset_version=11\n",
    ")"
   ]
  },
  {
   "source": [
    "# 2. Running ONNX inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"models/hot_dog_resnet50_256_256.onnx\")\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "input_name, label_name # values defined at export "
   ]
  },
  {
   "source": [
    "## Converting image to tensor\n",
    "\n",
    "ONNX runtime has the advantage of small module footprint. We don't want to bloat it with PyTorch's transforms to convert the image to tensor as previously. We can use Pillow with numpy to load and prepare the image for inference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Image load & prep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open('dataset/hot_dog/hot_dog_1.jpg')\n",
    "image = image.resize((256,256))\n",
    "print(image.shape, image.mode)\n",
    "\n",
    "\n",
    "# now our image is represented by 3 layers - Red, Green, Blue\n",
    "# each layer has a 224 x 224 values representing\n",
    "image = np.array(image)\n",
    "print('Conversion to tensor: ',image.shape)\n",
    "\n",
    "# dummy input for the model at export - torch.randn(1, 3, 224, 224)\n",
    "image = image.transpose(2,0,1).astype(np.float32)\n",
    "print('Transposing the tensor: ',image.shape)\n",
    "\n",
    "# our image is currently represented by values ranging between 0-255\n",
    "# we need to convert these values to 0.0-1.0 - those are the values that are expected by our model\n",
    "\n",
    "print('Integer value: ', image[0][0][40])\n",
    "image /= 255\n",
    "print('Float value: ', image[0][0][40])\n",
    "\n",
    "# expanding the alread existing tensor with the final dimension (similar to unsqueeze(0))\n",
    "# currently our tensor only has rank of 3 which needs to be expanded to 4 - torch.randn(1, 3, 224, 224)\n",
    "# 1 can be considered the batch size\n",
    "\n",
    "image = image[None, ...]\n",
    "print('Final shape of our tensor', image.shape)\n"
   ]
  },
  {
   "source": [
    "## Run inference with ONNX Runtime"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession('models/hot_dog_resnet50_256_256.onnx')\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "input_name, output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sess.run([output_name], {input_name: image})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.argmax(results)], results, labels"
   ]
  },
  {
   "source": [
    "## Gotchas\n",
    "\n",
    "It's really important to use the expected input with ONNX. Let's check the following scenario when using a 1x3x224x224 tensor with on a model with a defined input of 1x3x256x256.\n",
    "\n",
    "Let's see what happens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open('dataset/hot_dog/hot_dog_1.jpg')\n",
    "image = image.resize((224,224)) # <---- values changed here\n",
    "print(image.shape, image.mode)\n",
    "\n",
    "image = np.array(image)\n",
    "print('Conversion to tensor: ',image.shape)\n",
    "\n",
    "image = image.transpose(2,0,1).astype(np.float32)\n",
    "print('Transposing the tensor: ',image.shape)\n",
    "\n",
    "print('Integer value: ', image[0][0][40])\n",
    "image /= 255\n",
    "print('Float value: ', image[0][0][40])\n",
    "\n",
    "image = image[None, ...]\n",
    "print('Final shape of our tensor', image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sess.run([output_name], {input_name: image})[0]\n",
    "# an ERROR is expected - no worries"
   ]
  },
  {
   "source": [
    "## How to Debug\n",
    "\n",
    "To check the inputs of a model you can use a tool like Netron to visualize it: https://netron.app. Desktop version available here: https://github.com/lutzroeder/netron\n",
    "\n",
    "OR you can access the expected dimension by the following line:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the required model input\n",
    "sess.get_inputs()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}